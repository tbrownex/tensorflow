{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read a csv file and convert it to a TF binary\n",
    "https://indico.io/blog/tensorflow-data-inputs-part1-placeholders-protobufs-queues/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 records and 3 columns\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/tom/data/haganCaseStudy1.csv\")\n",
    "print('{} records and {} columns'.format(df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get each feature, and the Target, in its own series\n",
    "df['num'] = range(1, len(df) + 1)             # Add the example number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num = df['num'].astype('float')\n",
    "v1 = df['v1']\n",
    "v2 = df['v2']\n",
    "Y = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a RecordWriter and load Example. Example requires an iterable, so one write \n",
    "# will take the whole column from the dataframe\n",
    "# The output is a binary file\n",
    "writer = tf.python_io.TFRecordWriter(\"/home/tom/data/haganCaseStudy1.tfrecords\")\n",
    "\n",
    "example = tf.train.Example(features=tf.train.Features(\n",
    "        feature={\n",
    "            'num': tf.train.Feature(float_list=tf.train.FloatList(value=num)),\n",
    "            'v1': tf.train.Feature(float_list=tf.train.FloatList(value=v1)),\n",
    "            'v2': tf.train.Feature(float_list=tf.train.FloatList(value=v2)),\n",
    "            'Y': tf.train.Feature(float_list=tf.train.FloatList(value=Y))\n",
    "        }))\n",
    "\n",
    "serialized = example.SerializeToString()\n",
    "writer.write(serialized)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now read it in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "DATA_LOC  = \"/home/tom/data/\"\n",
    "DATA_FILE = \"haganCaseStudy1.tfrecords\"\n",
    "BATCH_SIZE    = 2\n",
    "TEST_PCT      = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"size\" is the number of records in the input file\n",
    "# \"pct\" is the percent of records to make Test records\n",
    "def create_mask(size, pct):\n",
    "    mask = [0 for x in range(size)]\n",
    "    test_set_size = int(size*pct)\n",
    "    mask[:test_set_size] = [1] * test_set_size\n",
    "    random.shuffle(mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the data and load Features (v1,v2) and Target (y)\n",
    "for serialized_rec in tf.python_io.tf_record_iterator(DATA_LOC + DATA_FILE):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(serialized_rec)\n",
    "    a = example.features.feature['num'].float_list.value\n",
    "    b = example.features.feature['v1'].float_list.value\n",
    "    c = example.features.feature['v2'].float_list.value\n",
    "    d = example.features.feature['Y'].float_list.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a single Tensor with the data\n",
    "num = tf.placeholder(\"float\", shape=[None])\n",
    "v1  = tf.placeholder(\"float\", shape=[None])\n",
    "v2  = tf.placeholder(\"float\", shape=[None])\n",
    "y   = tf.placeholder(\"float\", shape=[None])\n",
    "\n",
    "t_list = [num,v1,v2,y]\n",
    "T = tf.pack(t_list, axis=1)          # Merge into a single tensor\n",
    "\n",
    "# Create a mask that randomizes the test/train sets\n",
    "mask = create_mask(len(a), TEST_PCT)\n",
    "\n",
    "# Split the data into Train and Test\n",
    "train, test = tf.dynamic_partition(T, mask, 2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    tr,te = sess.run([train, test], feed_dict = {num:a, v1:b, v2:c, y:d})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set rows: 51\n",
      "Testing  set rows: 16\n"
     ]
    }
   ],
   "source": [
    "print('Training set rows: {}'.format(tr.shape[0]))\n",
    "print('Testing  set rows: {}'.format(te.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
